{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01363a10-eb06-4948-abfa-9fb591777409",
   "metadata": {
    "id": "01363a10-eb06-4948-abfa-9fb591777409"
   },
   "source": [
    "## 1. Why and When Do We Need It?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc07d50-7a63-48c3-935f-c7ebde646f34",
   "metadata": {
    "id": "8dc07d50-7a63-48c3-935f-c7ebde646f34"
   },
   "source": [
    "* **Patent data** has been a primary source for monitoring technological advancement and innovation activities; however, it has a limited ability to portray **market-side information**.\n",
    "  \n",
    "* To overcome this, existing studies have relied on **web mining techniques** to analyze companies’ web content, which companies commonly use to exhibit their **commercial activities**, including **main products and services, key technologies,** and personnel.\n",
    "  \n",
    "* However, company websites often include **miscellaneous information**, ranging from primary business information (i.e., products and services) to more general information (i.e., factory sites and recruitment). The noisy information embedded in the raw web data may blur the following analysis.\n",
    "  \n",
    "* The dual-attention model is designed to **automatically identify technical information** (i.e., products or firms’ technological capabilities) from the raw web data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d070fd1a-2201-405c-bd32-a7565ee4396f",
   "metadata": {
    "id": "d070fd1a-2201-405c-bd32-a7565ee4396f"
   },
   "source": [
    "## 2. The Structure of the Dual-Attention Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127d319c-838a-4697-8c26-6b77545c367d",
   "metadata": {
    "id": "127d319c-838a-4697-8c26-6b77545c367d"
   },
   "source": [
    "* Two attention layers are applied at the **webpage-level** and **word-level**, respectively.\n",
    "\n",
    "* The training target is a **binary objective** of **high-tech versus non-high-tech companies**. High-tech companies are defined as companies with at least one patent in this study.\n",
    "\n",
    "* The **assumption**: the terminologies presented on the websites of high-tech companies are distributed heterogeneously among\n",
    "their non-high-tech counterparts (i.e., food companies and wholesalers).\n",
    "\n",
    "* After the training process, both word-level and page-level attention scores will be extracted for the following information selection process.\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"./dual_attn_model.png\" width=\"800\" height=\"400\">\n",
    "</div>\n",
    "\n",
    "\n",
    "* **Sparsemax vs. Softmax:** In contrast to the conventional softmax function, the sparsemax function converts real values into sparse probabilities. Because a company's website usually covers miscellaneous webpages, using sparsemax enables the algorithm to be more concentrated on crucial webpages, which also helps save one parameter for the following keyword extraction process.\n",
    "\n",
    "* **Reference**: [Identifying technology opportunity using dual-attention model and technology-market concordance matrix.](https://www.sciencedirect.com/science/article/abs/pii/S0040162523006017)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71068443-047e-4f46-869e-15d60abc6fb4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390
    },
    "executionInfo": {
     "elapsed": 13112,
     "status": "error",
     "timestamp": 1719125661931,
     "user": {
      "displayName": "Abhishek Krishnan ee22s300",
      "userId": "15429915476175442778"
     },
     "user_tz": -330
    },
    "id": "71068443-047e-4f46-869e-15d60abc6fb4",
    "outputId": "316fbf5f-f1a0-4fa8-ace5-0682d969bbb3"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from tokenizer import Tokenizer\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import confusion_matrix, f1_score, recall_score, precision_score\n",
    "from dual_attn import DualAttnModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ae239c-b0a9-45cf-bef0-0a09407bd115",
   "metadata": {
    "id": "c0ae239c-b0a9-45cf-bef0-0a09407bd115"
   },
   "source": [
    "#### Step 1. Load the data\n",
    "\n",
    "* When loading your own data, make sure the **column names** match those provided in the example dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4719d779-215e-4164-b2f6-6d85b04684b0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 161
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "error",
     "timestamp": 1719156073347,
     "user": {
      "displayName": "Abhishek Krishnan ee22s300",
      "userId": "15429915476175442778"
     },
     "user_tz": -330
    },
    "id": "4719d779-215e-4164-b2f6-6d85b04684b0",
    "outputId": "f1c70f6b-081a-4062-a9f3-66679b4de0f4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>company_name</th>\n",
       "      <th>urls</th>\n",
       "      <th>cleaned_content</th>\n",
       "      <th>hojin_id</th>\n",
       "      <th>hightechflag</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>IN*110022183105</td>\n",
       "      <td>http://www.shaktipumps.com/pdf/Investor_Relati...</td>\n",
       "      <td>streamhj</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>IN*110022183105</td>\n",
       "      <td>https://www.shaktipumps.com/branch-offices.php</td>\n",
       "      <td>shakti|branch|offices||location|branch|shakti|...</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>IN*110022183105</td>\n",
       "      <td>https://www.shaktipumps.com/csr-activity.php</td>\n",
       "      <td>company|activity-|shakti|limited|blower|homeco...</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>IN*110022183105</td>\n",
       "      <td>https://www.shaktipumps.com</td>\n",
       "      <td>shakti|manufacturer|supplier|blower|homecompan...</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>IN*110022183105</td>\n",
       "      <td>http://www.shaktipumps.com/media/promotions/de...</td>\n",
       "      <td>blower|homecompany|closecompany|profilevision|...</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244174</th>\n",
       "      <td>244174</td>\n",
       "      <td>IN0007502693</td>\n",
       "      <td>http://www.lexistoolingsystems.com/straight-co...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23199</td>\n",
       "      <td>0</td>\n",
       "      <td>128739.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244175</th>\n",
       "      <td>244175</td>\n",
       "      <td>IN0007502693</td>\n",
       "      <td>http://www.lexistoolingsystems.com/sitemap.html</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23199</td>\n",
       "      <td>0</td>\n",
       "      <td>128740.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244176</th>\n",
       "      <td>244176</td>\n",
       "      <td>IN0007502693</td>\n",
       "      <td>http://www.lexistoolingsystems.com/er-tap-coll...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23199</td>\n",
       "      <td>0</td>\n",
       "      <td>128741.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244177</th>\n",
       "      <td>244177</td>\n",
       "      <td>IN0007502693</td>\n",
       "      <td>http://www.lexistoolingsystems.com/er-wrench-s...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23199</td>\n",
       "      <td>0</td>\n",
       "      <td>128742.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244178</th>\n",
       "      <td>244178</td>\n",
       "      <td>IN0007502693</td>\n",
       "      <td>http://www.lexistoolingsystems.com/industrial-...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23199</td>\n",
       "      <td>0</td>\n",
       "      <td>128743.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>244179 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0.1     company_name  \\\n",
       "0                  0  IN*110022183105   \n",
       "1                  1  IN*110022183105   \n",
       "2                  2  IN*110022183105   \n",
       "3                  3  IN*110022183105   \n",
       "4                  4  IN*110022183105   \n",
       "...              ...              ...   \n",
       "244174        244174     IN0007502693   \n",
       "244175        244175     IN0007502693   \n",
       "244176        244176     IN0007502693   \n",
       "244177        244177     IN0007502693   \n",
       "244178        244178     IN0007502693   \n",
       "\n",
       "                                                     urls  \\\n",
       "0       http://www.shaktipumps.com/pdf/Investor_Relati...   \n",
       "1          https://www.shaktipumps.com/branch-offices.php   \n",
       "2            https://www.shaktipumps.com/csr-activity.php   \n",
       "3                             https://www.shaktipumps.com   \n",
       "4       http://www.shaktipumps.com/media/promotions/de...   \n",
       "...                                                   ...   \n",
       "244174  http://www.lexistoolingsystems.com/straight-co...   \n",
       "244175    http://www.lexistoolingsystems.com/sitemap.html   \n",
       "244176  http://www.lexistoolingsystems.com/er-tap-coll...   \n",
       "244177  http://www.lexistoolingsystems.com/er-wrench-s...   \n",
       "244178  http://www.lexistoolingsystems.com/industrial-...   \n",
       "\n",
       "                                          cleaned_content  hojin_id  \\\n",
       "0                                                streamhj     10000   \n",
       "1       shakti|branch|offices||location|branch|shakti|...     10000   \n",
       "2       company|activity-|shakti|limited|blower|homeco...     10000   \n",
       "3       shakti|manufacturer|supplier|blower|homecompan...     10000   \n",
       "4       blower|homecompany|closecompany|profilevision|...     10000   \n",
       "...                                                   ...       ...   \n",
       "244174                                                NaN     23199   \n",
       "244175                                                NaN     23199   \n",
       "244176                                                NaN     23199   \n",
       "244177                                                NaN     23199   \n",
       "244178                                                NaN     23199   \n",
       "\n",
       "        hightechflag  Unnamed: 0  \n",
       "0                  1         NaN  \n",
       "1                  1         NaN  \n",
       "2                  1         NaN  \n",
       "3                  1         NaN  \n",
       "4                  1         NaN  \n",
       "...              ...         ...  \n",
       "244174             0    128739.0  \n",
       "244175             0    128740.0  \n",
       "244176             0    128741.0  \n",
       "244177             0    128742.0  \n",
       "244178             0    128743.0  \n",
       "\n",
       "[244179 rows x 7 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_excel(\"fpatent.xlsx\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7d0b6c-31a4-492f-b667-90985a72e855",
   "metadata": {
    "id": "bd7d0b6c-31a4-492f-b667-90985a72e855"
   },
   "source": [
    "#### Step 2. Load the pretrained word vectors\n",
    "\n",
    "* To initialize the dual-attention model, we need to use the pre-trained word vectors.\n",
    "\n",
    "* For example, one can download different pre-trained word vectors provided by [FastText](https://fasttext.cc/docs/en/crawl-vectors.html).\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"./pretrainedftt.png\" width=\"800\" height=\"400\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0e9d959-fc31-41d3-ab15-3bc44e2de797",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class FastVector:\n",
    "    \"\"\"\n",
    "    Minimal wrapper for fastvector embeddings.\n",
    "    ```\n",
    "    Usage:\n",
    "        $ model = FastVector(vector_file='/path/to/wiki.en.vec')\n",
    "        $ 'apple' in model\n",
    "        > TRUE\n",
    "        $ model['apple'].shape\n",
    "        > (300,)\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vector_file='', transform=None):\n",
    "        \"\"\"Read in word vectors in fasttext format\"\"\"\n",
    "        self.word2id = {}\n",
    "\n",
    "        # Captures word order, for export() and translate methods\n",
    "        self.id2word = []\n",
    "\n",
    "        print('reading word vectors from %s' % vector_file)\n",
    "        with open(vector_file, 'r', encoding='utf-8') as f:\n",
    "            print(f)\n",
    "            (self.n_words, self.n_dim) = \\\n",
    "                (int(x) for x in f.readline().rstrip('\\n').split(' '))\n",
    "            self.embed = np.zeros((self.n_words, self.n_dim))\n",
    "            for i, line in enumerate(f):\n",
    "                elems = line.rstrip('\\n').split(' ')\n",
    "                self.word2id[elems[0]] = i\n",
    "                self.embed[i] = elems[1:self.n_dim+1]\n",
    "                self.id2word.append(elems[0])\n",
    "        \n",
    "        # Used in translate_inverted_softmax()\n",
    "        self.softmax_denominators = None\n",
    "        \n",
    "        if transform is not None:\n",
    "            print('Applying transformation to embedding')\n",
    "            self.apply_transform(transform)\n",
    "\n",
    "    def apply_transform(self, transform):\n",
    "        \"\"\"\n",
    "        Apply the given transformation to the vector space\n",
    "\n",
    "        Right-multiplies given transform with embeddings E:\n",
    "            E = E * transform\n",
    "\n",
    "        Transform can either be a string with a filename to a\n",
    "        text file containing a ndarray (compat. with np.loadtxt)\n",
    "        or a numpy ndarray.\n",
    "        \"\"\"\n",
    "        transmat = np.loadtxt(transform) if isinstance(transform, str) else transform\n",
    "        self.embed = np.matmul(self.embed, transmat)\n",
    "\n",
    "    def export(self, outpath):\n",
    "        \"\"\"\n",
    "        Transforming a large matrix of WordVectors is expensive. \n",
    "        This method lets you write the transformed matrix back to a file for future use\n",
    "        :param The path to the output file to be written \n",
    "        \"\"\"\n",
    "        fout = open(outpath, \"w\")\n",
    "\n",
    "        # Header takes the guesswork out of loading by recording how many lines, vector dims\n",
    "        fout.write(str(self.n_words) + \" \" + str(self.n_dim) + \"\\n\")\n",
    "        for token in self.id2word:\n",
    "            vector_components = [\"%.6f\" % number for number in self[token]]\n",
    "            vector_as_string = \" \".join(vector_components)\n",
    "\n",
    "            out_line = token + \" \" + vector_as_string + \"\\n\"\n",
    "            fout.write(out_line)\n",
    "\n",
    "        fout.close()\n",
    "\n",
    "    def translate_nearest_neighbour(self, source_vector):\n",
    "        \"\"\"Obtain translation of source_vector using nearest neighbour retrieval\"\"\"\n",
    "        similarity_vector = np.matmul(FastVector.normalised(self.embed), source_vector)\n",
    "        target_id = np.argmax(similarity_vector)\n",
    "        return self.id2word[target_id]\n",
    "\n",
    "    def translate_inverted_softmax(self, source_vector, source_space, nsamples,\n",
    "                                   beta=10., batch_size=100, recalculate=True):\n",
    "        \"\"\"\n",
    "        Obtain translation of source_vector using sampled inverted softmax retrieval\n",
    "        with inverse temperature beta.\n",
    "\n",
    "        nsamples vectors are drawn from source_space in batches of batch_size\n",
    "        to calculate the inverted softmax denominators.\n",
    "        Denominators from previous call are reused if recalculate=False. This saves\n",
    "        time if multiple words are translated from the same source language.\n",
    "        \"\"\"\n",
    "        embed_normalised = FastVector.normalised(self.embed)\n",
    "        # calculate contributions to softmax denominators in batches\n",
    "        # to save memory\n",
    "        if self.softmax_denominators is None or recalculate is True:\n",
    "            self.softmax_denominators = np.zeros(self.embed.shape[0])\n",
    "            while nsamples > 0:\n",
    "                # get batch of randomly sampled vectors from source space\n",
    "                sample_vectors = source_space.get_samples(min(nsamples, batch_size))\n",
    "                # calculate cosine similarities between sampled vectors and\n",
    "                # all vectors in the target space\n",
    "                sample_similarities = \\\n",
    "                    np.matmul(embed_normalised,\n",
    "                              FastVector.normalised(sample_vectors).transpose())\n",
    "                # accumulate contribution to denominators\n",
    "                self.softmax_denominators \\\n",
    "                    += np.sum(np.exp(beta * sample_similarities), axis=1)\n",
    "                nsamples -= batch_size\n",
    "        # cosine similarities between source_vector and all target vectors\n",
    "        similarity_vector = np.matmul(embed_normalised,\n",
    "                                      source_vector/np.linalg.norm(source_vector))\n",
    "        # exponentiate and normalise with denominators to obtain inverted softmax\n",
    "        softmax_scores = np.exp(beta * similarity_vector) / \\\n",
    "                         self.softmax_denominators\n",
    "        # pick highest score as translation\n",
    "        target_id = np.argmax(softmax_scores)\n",
    "        return self.id2word[target_id]\n",
    "\n",
    "    def get_samples(self, nsamples):\n",
    "        \"\"\"Return a matrix of nsamples randomly sampled vectors from embed\"\"\"\n",
    "        sample_ids = np.random.choice(self.embed.shape[0], nsamples, replace=False)\n",
    "        return self.embed[sample_ids]\n",
    "\n",
    "    @classmethod\n",
    "    def normalised(cls, mat, axis=-1, order=2):\n",
    "        \"\"\"Utility function to normalise the rows of a numpy array.\"\"\"\n",
    "        norm = np.linalg.norm(\n",
    "            mat, axis=axis, ord=order, keepdims=True)\n",
    "        norm[norm == 0] = 1\n",
    "        return mat / norm\n",
    "    \n",
    "    @classmethod\n",
    "    def cosine_similarity(cls, vec_a, vec_b):\n",
    "        \"\"\"Compute cosine similarity between vec_a and vec_b\"\"\"\n",
    "        return np.dot(vec_a, vec_b) / \\\n",
    "            (np.linalg.norm(vec_a) * np.linalg.norm(vec_b))\n",
    "\n",
    "    def __contains__(self, key):\n",
    "        return key in self.word2id\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return self.embed[self.word2id[key]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe87cb32-370d-4ab6-b184-b5766e2b8630",
   "metadata": {
    "id": "fe87cb32-370d-4ab6-b184-b5766e2b8630",
    "outputId": "14914b85-c825-4af9-a302-f8c9251092d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading word vectors from cc.en.300.vec\n",
      "<_io.TextIOWrapper name='cc.en.300.vec' mode='r' encoding='utf-8'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "en_dictionary = FastVector(vector_file='cc.en.300.vec')\n",
    "\n",
    "words = list(en_dictionary.word2id.keys())\n",
    "vectors = np.array([en_dictionary[word] for word in words])\n",
    "\n",
    "wv_dict = dict(zip(words, vectors))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6120fb-2215-4072-93df-e4262b3e26cb",
   "metadata": {
    "id": "2d6120fb-2215-4072-93df-e4262b3e26cb"
   },
   "source": [
    "#### Step 3. Preprocess the data\n",
    "\n",
    "* Remove bad entries.\n",
    "* Set the **max_page** parameter for cutting and padding strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85540484-3fe0-446f-a256-c80ebb50930e",
   "metadata": {
    "id": "85540484-3fe0-446f-a256-c80ebb50930e",
    "outputId": "a2c157e9-7e2a-4d03-e23b-516de6ded61e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 244179/244179 [00:12<00:00, 18874.97it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 5358/5358 [00:14<00:00, 372.80it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 5194/5194 [00:09<00:00, 571.67it/s]\n"
     ]
    }
   ],
   "source": [
    "no_values = []\n",
    "\n",
    "for i in tqdm(data.cleaned_content):\n",
    "    try:\n",
    "        i = i.split('|')\n",
    "        i = [j for j in i if j in wv_dict]\n",
    "        if len(i) < 1:\n",
    "            no_values.append(1)\n",
    "        else:\n",
    "            no_values.append(0)\n",
    "    except:\n",
    "        no_values.append(1)\n",
    "\n",
    "\n",
    "data['no_values'] = no_values\n",
    "data = data[data.no_values == 0]\n",
    "data = data[['hojin_id', 'company_name', 'urls', 'cleaned_content', 'hightechflag']]\n",
    "\n",
    "\n",
    "hojin_ids = list(set(data.hojin_id))\n",
    "\n",
    "sample_data = pd.DataFrame({})\n",
    "max_page = 32\n",
    "\n",
    "for hojin_id in tqdm(hojin_ids):\n",
    "    temp = data[data.hojin_id == hojin_id]\n",
    "    if temp.shape[0] <= max_page:\n",
    "        sample_data = pd.concat([sample_data, temp], ignore_index=True)\n",
    "    else:\n",
    "        temp = temp.sample(n=max_page)\n",
    "        sample_data = pd.concat([sample_data, temp], ignore_index=True)\n",
    "        #sample_data = pd.concat([sample_data, temp.iloc[:max_page, :]], ignore_index=True)\n",
    "\n",
    "num_words = [len(i.split('|')) for i in sample_data.cleaned_content]\n",
    "sample_data['num_words'] = num_words\n",
    "sample_data = sample_data[sample_data.num_words > 5]\n",
    "\n",
    "hojin_ids = list(set(sample_data.hojin_id))\n",
    "hojin_ids = [int(i) for i in hojin_ids]\n",
    "\n",
    "tokenizer = Tokenizer(words, max_len=864, data = sample_data)\n",
    "\n",
    "web_vectors = [tokenizer.encode_webportfolio(company_id=idx, max_page=max_page) for idx in tqdm(hojin_ids)]\n",
    "\n",
    "seq_ids = torch.tensor([i[1] for i in web_vectors])\n",
    "num_pages = torch.tensor([i[0] for i in web_vectors])\n",
    "seq_lengths = tokenizer.max_len - torch.sum(seq_ids == 0, axis=-1)\n",
    "\n",
    "labels = torch.tensor([tokenizer.get_label(i) for i in hojin_ids])\n",
    "hojin_ids = torch.tensor(hojin_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800ab790-3651-44e0-8611-43b967a55e04",
   "metadata": {
    "id": "800ab790-3651-44e0-8611-43b967a55e04"
   },
   "source": [
    "#### Step 4. Initialize the dual-attention model and training settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d2fb0af-64b4-4cb5-a8cc-28b9f2c368ec",
   "metadata": {
    "id": "7d2fb0af-64b4-4cb5-a8cc-28b9f2c368ec",
    "outputId": "aa57392d-8e9d-4a59-bd34-2e23c247f728"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings loaded\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "dataset = TensorDataset(seq_ids, num_pages, seq_lengths, labels, hojin_ids)\n",
    "train_dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "def evaluate(data_loader, model):\n",
    "    model.eval()\n",
    "    count = 0\n",
    "    i = 0\n",
    "    gold_labels = []\n",
    "    pred_labels = []\n",
    "    for ind, batch in tqdm(enumerate(data_loader), ncols=80):\n",
    "\n",
    "        seq_ids, num_pages, seq_lengths, label_list, hojin = batch\n",
    "        outputs, _, _, _, _, _, _ = model(seq_ids.to(device), num_pages.to(device), seq_lengths.to(device))\n",
    "        preds = (outputs>0.5).squeeze()\n",
    "\n",
    "        gold_labels += list(label_list.cpu().numpy())\n",
    "        pred_labels += list(preds.cpu().numpy())\n",
    "        num = (preds.cpu() == label_list.bool()).sum().cpu().item()\n",
    "        count += num\n",
    "        i += 1\n",
    "    accuracy = count*1.0/(i * batch_size)\n",
    "    print('Evaluation accuracy:', accuracy)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "\n",
    "vectors = np.array(list(wv_dict.values()))\n",
    "words = list(wv_dict.keys())\n",
    "vectors_all = np.vstack([np.zeros(300), vectors])\n",
    "\n",
    "torch.manual_seed(1218)\n",
    "loss_function = nn.BCELoss()\n",
    "scale = 10\n",
    "\n",
    "model = DualAttnModel(vocab_size=len(words)+1, embed_dim=300, hidden_dim=300,\n",
    "                             label_dim=1, scale=10, page_scale=10)\n",
    "model.load_vector(pretrained_vectors=vectors_all, trainable=False)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adagrad(model.parameters(), lr=0.02, weight_decay=0.0000, lr_decay=0.01) # try different learning rates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf96afeb-4d8a-4064-9dfa-d4f3be5291cb",
   "metadata": {
    "id": "bf96afeb-4d8a-4064-9dfa-d4f3be5291cb"
   },
   "source": [
    "#### Step 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2103695b-3c97-48dc-9cbb-9287c82a280f",
   "metadata": {
    "id": "2103695b-3c97-48dc-9cbb-9287c82a280f",
    "outputId": "5eaaa569-1bdd-4830-e514-d3eff824a830"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "####################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "520it [00:13, 38.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_loss:   0.7498329973790019\n",
      "Training Accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "520it [00:11, 44.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation accuracy: 0.5340384615384616\n",
      "Epoch: 1\n",
      "####################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "520it [00:12, 42.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_loss:   0.7247671073429576\n",
      "Training Accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "520it [00:10, 49.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation accuracy: 0.5340384615384616\n",
      "Epoch: 2\n",
      "####################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "520it [00:11, 44.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_loss:   0.7216899127261566\n",
      "Training Accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "520it [00:10, 50.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation accuracy: 0.5340384615384616\n",
      "Epoch: 3\n",
      "####################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "520it [00:12, 41.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_loss:   0.7214595958917565\n",
      "Training Accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "520it [00:11, 46.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation accuracy: 0.5340384615384616\n",
      "Epoch: 4\n",
      "####################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "520it [00:12, 40.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_loss:   0.7205519180459429\n",
      "Training Accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "520it [00:10, 51.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation accuracy: 0.5340384615384616\n",
      "Epoch: 5\n",
      "####################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "520it [00:11, 45.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_loss:   0.7209792135195132\n",
      "Training Accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "520it [00:10, 49.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation accuracy: 0.5340384615384616\n",
      "Epoch: 6\n",
      "####################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "520it [00:11, 43.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_loss:   0.7202679919058147\n",
      "Training Accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "304it [00:07, 54.67it/s]"
     ]
    }
   ],
   "source": [
    "best_model = None\n",
    "best_acc = 0\n",
    "for i in range(10):\n",
    "    print('Epoch:', i)\n",
    "    print('#'*20)\n",
    "    total_loss = 0\n",
    "    count = 0\n",
    "    model.train()\n",
    "    for ind, batch in tqdm(enumerate(train_dataloader), ncols=80):\n",
    "        seq_ids, num_pages, seq_lengths, label_list, hojin = batch\n",
    "        model.zero_grad()\n",
    "        preds, _, _, _, _, _, _ = model(seq_ids.to(device), num_pages.to(device), seq_lengths.to(device))\n",
    "        loss = loss_function(preds.squeeze(), label_list.to(device).float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.cpu().item()*len(seq_ids)\n",
    "        count += len(seq_ids)\n",
    "    print('total_loss:  ', total_loss/count)\n",
    "    print('Training Accuracy')\n",
    "    evaluate(train_dataloader, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0648e88-faa6-411e-963b-0a4e3f5bc88d",
   "metadata": {
    "id": "f0648e88-faa6-411e-963b-0a4e3f5bc88d"
   },
   "source": [
    "#### Step 6. Collect the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65f4c95-38fa-473b-8e98-bdeebfca0cce",
   "metadata": {
    "id": "f65f4c95-38fa-473b-8e98-bdeebfca0cce",
    "outputId": "5ebdb379-ca62-4fbc-d9aa-e977d085f3e2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 20/20 [00:00<00:00, 162.31it/s]\n"
     ]
    }
   ],
   "source": [
    "words = list(en_dictionary.word2id.keys())\n",
    "vectors = np.array([en_dictionary[word] for word in words])\n",
    "\n",
    "wv_dict = dict(zip(words, vectors))\n",
    "\n",
    "hojin_ids = list(set(sample_data.hojin_id))\n",
    "hojin_ids = [int(i) for i in hojin_ids]\n",
    "\n",
    "tokenizer = Tokenizer(words, max_len=864, data = sample_data)\n",
    "\n",
    "web_vectors = [tokenizer.encode_webportfolio(company_id=idx, max_page=max_page) for idx in tqdm(hojin_ids)]\n",
    "\n",
    "seq_ids = torch.tensor([i[1] for i in web_vectors])\n",
    "num_pages = torch.tensor([i[0] for i in web_vectors])\n",
    "seq_lengths = tokenizer.max_len - torch.sum(seq_ids == 0, axis=-1)\n",
    "labels = torch.tensor([tokenizer.get_label(i) for i in hojin_ids])\n",
    "hojin_ids = torch.tensor(hojin_ids)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def colorize(words, color_array):\n",
    "    cmap=matplotlib.cm.Blues\n",
    "    template = '<span class=\"barcode\"; style=\"color: black; background-color: {}\">{}</span>'\n",
    "    colored_string = ''\n",
    "    for word, color in zip(words, color_array):\n",
    "        color = matplotlib.colors.rgb2hex(cmap(color)[:3])\n",
    "        print(color)\n",
    "        colored_string += template.format(color, '&nbsp' + word + '&nbsp')\n",
    "    return colored_string\n",
    "\n",
    "def select_keywords(attn_w, words, n=10):\n",
    "    combo = [(i, j) for i, j in zip(attn_w, words) if i != 0]\n",
    "    attn_w = np.array([i[0] for i in combo])\n",
    "    words = [i[1] for i in combo]\n",
    "    attn_diff = attn_w.max() - attn_w\n",
    "    attn_thres = np.percentile(attn_diff, n)\n",
    "    selected_keywords = [i for i, j in zip(words, attn_diff) if j <= attn_thres]\n",
    "    selected_keywords_show = [0.6 if j <= attn_thres else 0 for i, j in zip(words, attn_diff)]\n",
    "    return selected_keywords, selected_keywords_show\n",
    "\n",
    "url_col = []\n",
    "text_col = []\n",
    "sents_selected = []\n",
    "weight_col = []\n",
    "hojin_id_col = []\n",
    "hightechflag_col = []\n",
    "model.eval()\n",
    "\n",
    "final_vecs = []\n",
    "web_vecs = []\n",
    "page_attns = []\n",
    "urls = []\n",
    "\n",
    "\n",
    "for t in range(len(hojin_ids)):\n",
    "\n",
    "    probs, senti_scores, attn, page_attn, final_vec, page_score, web_vec = model(seq_ids[t:(t+1)].to(device), num_pages[t:(t+1)].to(device), seq_lengths[t:(t+1)].to(device))\n",
    "    id_to_token = tokenizer.id_to_token\n",
    "    id_to_token[0] = '#'\n",
    "\n",
    "\n",
    "    sents = []\n",
    "    for i in range(num_pages[t:(t+1)].tolist()[0]):\n",
    "        sents.append(' '.join([id_to_token[w] for w in seq_ids[t:(t+1)][0][i].tolist()]))\n",
    "\n",
    "    final_vecs.append(final_vec.detach().cpu().numpy())\n",
    "    df = pd.DataFrame({'url': list(sample_data[sample_data.hojin_id == int(hojin_ids[t].tolist())].urls),\n",
    "                   'hojin_id': list(sample_data[sample_data.hojin_id == int(hojin_ids[t].tolist())].hojin_id),\n",
    "                   'hightechflag': list(sample_data[sample_data.hojin_id == int(hojin_ids[t].tolist())].hightechflag),\n",
    "                   'text':list(sample_data[sample_data.hojin_id == int(hojin_ids[t].tolist())].cleaned_content),\n",
    "                   'weight': page_attn.view(-1)[:num_pages[t:(t+1)].tolist()[0]].tolist(),\n",
    "                   'page_score': page_score.view(-1)[page_score.view(-1) > -9999].tolist(),\n",
    "                   #'web_vecs': list(web_vec[0])[:num_pages[t:(t+1)].tolist()[0]]\n",
    "                   })\n",
    "    df = df[df.weight > 0].reset_index()\n",
    "\n",
    "\n",
    "    hojin_id_col.extend(df.hojin_id)\n",
    "    hightechflag_col.extend(df.hightechflag)\n",
    "    url_col.extend(df.url)\n",
    "    text_col.extend(df.text)\n",
    "    weight_col.extend(df.weight)\n",
    "\n",
    "    for i in list(df['index']):\n",
    "        sent = sents[i]\n",
    "        attn1 = attn.squeeze()[i]\n",
    "\n",
    "        words = sent.split()\n",
    "        color_array = np.array(attn1.view(-1).tolist())\n",
    "\n",
    "        selected_keywords, selected_keywords_show = select_keywords(color_array, words, n=20)\n",
    "\n",
    "        sents_selected.append([j for j, k in zip(words, selected_keywords_show) if k != 0])\n",
    "\n",
    "\n",
    "sents_selected = ['|'.join(i) for i in sents_selected]\n",
    "sents_selected = ['|'.join(list(set(i.split('|')))) for i in sents_selected]\n",
    "\n",
    "\n",
    "selected_df = pd.DataFrame({\n",
    "    'hojin_id': hojin_id_col,\n",
    "    'url': url_col,\n",
    "    'weight': weight_col,\n",
    "    'text':text_col,\n",
    "    'sents': sents_selected, 'hightechflag': hightechflag_col,\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b0fd24-033f-4a03-94ce-1a413184c7a4",
   "metadata": {
    "id": "c2b0fd24-033f-4a03-94ce-1a413184c7a4",
    "outputId": "d5ca70ae-4711-43da-cf84-f0f64b1d2da4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hojin_id</th>\n",
       "      <th>url</th>\n",
       "      <th>weight</th>\n",
       "      <th>text</th>\n",
       "      <th>sents</th>\n",
       "      <th>hightechflag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>704512</td>\n",
       "      <td>https://sbgi.net/sinclair-broadcast-group-name...</td>\n",
       "      <td>0.091239</td>\n",
       "      <td>menu|investor|relations|profile|sec|filings|fi...</td>\n",
       "      <td>via|phone|tulsa|billie|mail|expand|mass|im|net...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>704512</td>\n",
       "      <td>https://sbgi.net/join-sinclair/</td>\n",
       "      <td>0.100939</td>\n",
       "      <td>menu|join|sinclair|current|openings|view|job|l...</td>\n",
       "      <td>bally|open|fmla|dental|telehealth|leave|k|inc|...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>704512</td>\n",
       "      <td>https://sbgi.net/news/the-national-desk/</td>\n",
       "      <td>0.096508</td>\n",
       "      <td>menu|americas|news|national|desk|tnd|launched|...</td>\n",
       "      <td>rights|menu|subscription|inc|news|sinclair|ame...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>704512</td>\n",
       "      <td>https://sbgi.net/investor-relations/</td>\n",
       "      <td>0.093900</td>\n",
       "      <td>menu|investor|relations|profile|sec|filings|fi...</td>\n",
       "      <td>phone|grow|billie|download|mail|expand|network...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>704512</td>\n",
       "      <td>https://sbgi.net/</td>\n",
       "      <td>0.107802</td>\n",
       "      <td>diversified|media|company|broadcast|sports|mar...</td>\n",
       "      <td>drag|bally|sg|via|nhl|go|select|mass|networks|...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>704530</td>\n",
       "      <td>https://www.highcomarmor.com/product-category/...</td>\n",
       "      <td>0.039235</td>\n",
       "      <td>american|manufactured|ballistic|armor|solution...</td>\n",
       "      <td>ar|contact|locator|threat|registration|trauma|...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>704530</td>\n",
       "      <td>https://www.highcomarmor.com/product-category/...</td>\n",
       "      <td>0.023877</td>\n",
       "      <td>american|manufactured|ballistic|armor|solution...</td>\n",
       "      <td>ba|contact|locator|registration|ohio|outreach|...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>704530</td>\n",
       "      <td>https://www.highcomarmor.com/product-category/...</td>\n",
       "      <td>0.009736</td>\n",
       "      <td>american|manufactured|ballistic|armor|solution...</td>\n",
       "      <td>stp|locator|threat|registration|ohio|sa|outrea...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>704531</td>\n",
       "      <td>https://morgangroupholdingco.com/category/press/</td>\n",
       "      <td>0.443004</td>\n",
       "      <td>morgan|group|holding|company|home|sec|filings|...</td>\n",
       "      <td>p|non|date|officer|rye|no|fees|inc|of|j|to|yor...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>704531</td>\n",
       "      <td>https://morgangroupholdingco.com/#</td>\n",
       "      <td>0.556996</td>\n",
       "      <td>morgan|group|holding|company|home|sec|filings|...</td>\n",
       "      <td>projections|liable|update|date|no|fees|web|of|...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>302 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     hojin_id                                                url    weight  \\\n",
       "0      704512  https://sbgi.net/sinclair-broadcast-group-name...  0.091239   \n",
       "1      704512                    https://sbgi.net/join-sinclair/  0.100939   \n",
       "2      704512           https://sbgi.net/news/the-national-desk/  0.096508   \n",
       "3      704512               https://sbgi.net/investor-relations/  0.093900   \n",
       "4      704512                                  https://sbgi.net/  0.107802   \n",
       "..        ...                                                ...       ...   \n",
       "297    704530  https://www.highcomarmor.com/product-category/...  0.039235   \n",
       "298    704530  https://www.highcomarmor.com/product-category/...  0.023877   \n",
       "299    704530  https://www.highcomarmor.com/product-category/...  0.009736   \n",
       "300    704531   https://morgangroupholdingco.com/category/press/  0.443004   \n",
       "301    704531                 https://morgangroupholdingco.com/#  0.556996   \n",
       "\n",
       "                                                  text  \\\n",
       "0    menu|investor|relations|profile|sec|filings|fi...   \n",
       "1    menu|join|sinclair|current|openings|view|job|l...   \n",
       "2    menu|americas|news|national|desk|tnd|launched|...   \n",
       "3    menu|investor|relations|profile|sec|filings|fi...   \n",
       "4    diversified|media|company|broadcast|sports|mar...   \n",
       "..                                                 ...   \n",
       "297  american|manufactured|ballistic|armor|solution...   \n",
       "298  american|manufactured|ballistic|armor|solution...   \n",
       "299  american|manufactured|ballistic|armor|solution...   \n",
       "300  morgan|group|holding|company|home|sec|filings|...   \n",
       "301  morgan|group|holding|company|home|sec|filings|...   \n",
       "\n",
       "                                                 sents  hightechflag  \n",
       "0    via|phone|tulsa|billie|mail|expand|mass|im|net...           1.0  \n",
       "1    bally|open|fmla|dental|telehealth|leave|k|inc|...           1.0  \n",
       "2    rights|menu|subscription|inc|news|sinclair|ame...           1.0  \n",
       "3    phone|grow|billie|download|mail|expand|network...           1.0  \n",
       "4    drag|bally|sg|via|nhl|go|select|mass|networks|...           1.0  \n",
       "..                                                 ...           ...  \n",
       "297  ar|contact|locator|threat|registration|trauma|...           0.0  \n",
       "298  ba|contact|locator|registration|ohio|outreach|...           0.0  \n",
       "299  stp|locator|threat|registration|ohio|sa|outrea...           0.0  \n",
       "300  p|non|date|officer|rye|no|fees|inc|of|j|to|yor...           0.0  \n",
       "301  projections|liable|update|date|no|fees|web|of|...           0.0  \n",
       "\n",
       "[302 rows x 6 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_df"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
